\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}
\usepackage{listings}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 60, useFancyQuotes = FALSE)
library("MASS")
library("plotrix")
library("ggplot2")
library("utils")
library("visualFields")
@


%% -- Article metainformation (author, title, ...) -----------------------------

\author{Cord Huchzermeyer~\orcidlink{0000-0002-0408-8981}\\Department of Ophthalmology\\University Hospital Erlangen
   \And Iván Marín-Franch\\Computational Optometry}
\Plainauthor{Cord Huchzermeyer, Iván Marín-Franch}

\title{visualFields 1.0.7: Tools for Analyzing the Field of Vision in \proglang{R}}
\Plaintitle{visualFields 1.0.7: Tools for Analyzing the Field of Vision in R}
\Shorttitle{visualFields 1.0.7 in \proglang{R}}

\Abstract{
  The \pkg{visualFields} package is part of the Open Perimetry Initiative (OPI), 
  a standard for interfacing with visual field testing machines. This article 
  introduces the package, which provides tools for analyzing the field of vision, 
  including visualization, statistical analysis, and clinical interpretation of 
  visual-field loss and its change over time.
}

\Keywords{visualFields, Open Perimetry Initiative, visual field analysis, \proglang{R}}
\Plainkeywords{visualFields, Open Perimetry Initiative, visual field analysis, R}

\Address{
  Cord Huchzermeyer\\
  Department of Ophthalmology, University Hospital Erlangen \\
  E-mail: \email{c.huchzermeyer@posteo.de}\\
  \\
  Iván Marín-Franch\\
  Computational Optometry\\
  E-mail: \email{ivan@example.com}\\
  URL: \url{https://www.computationaloptometry.com/}
}

\begin{document}
%%\SweaveOpts{concordance=TRUE}
\lstset{breaklines=true, breakatwhitespace=true}


%% -- Introduction -------------------------------------------------------------

\section[Introduction: Visual Fields in R]{Introduction: Visual Fields} \label{sec:intro}

The Open Perimetry Initiative (OPI) \citep{IMF:Tur2012, IMF:Marin2022a} is an open standard for interfacing with visual field testing devices (perimeters). It defines a set of core functions that enable the construction of a wide range of visual field tests. As of October 2017, OPI is fully implemented on the Octopus 900 and partially supported on the Heidelberg Edge Perimeter, Kowa AP 7000, CrewT IMO, and Centervue Compass.

The \pkg{visualFields} package, developed as part of the OPI ecosystem, provides a suite of tools for the statistical analysis and visualization of visual field data. It offers a flexible framework to support the development and application of innovative methods for analyzing visual field loss and its progression over time, with applications in both research and clinical settings.
\subsection{What is the visual field?}

The visual field refers to the entire area that can be seen at a given moment, encompassing both central and peripheral vision. It is typically measured in degrees of visual angle, with the fixation point defined as the origin at $x = 0^\circ$ and $y = 0^\circ$. Locations temporal and superior to fixation are represented by positive angles, while those nasal and inferior are denoted by negative angles.

In healthy vision, most visual functions, particularly light sensitivity, decline with increasing eccentricity—i.e., distance from the center of fixation. This decline in differential light sensitivity forms a three-dimensional structure known as the hill of vision, also referred to as Traquair’s island of vision \citep{IMF:Traquair1938}. Figure~\ref{fig:hill} below shows a classic rendering of the hill of vision by Aulhorn and Harms \citep{IMF:Aul1967}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.75\textwidth]{AulhornHillOfVision.png}
\caption{\label{fig:hill} Hill of vision as rendered by Aulhorn \& Harms.}
\end{figure}

\subsection{What is perimetry?}

\emph{Perimetry} (visual field testing) uses psychophysical methods to quantify the visual field by measuring a patient's sensitivity to light differences at various locations within the field. Psychophysical tests allow for the scientific measurement of the relationship between physical stimulus strength and subjective perception. This is achieved by systematically varying the stimulus strength across discrete trials based on the observer's responses. The observer is typically restricted to a limited set of responses, and the stimuli are varied according to a predefined algorithm.

In visual field testing, the observer either presses or does not press a button within a fixed time window following the presentation of a light increment in the visual field (a yes/no response). The relationship between stimulus strength and response probability is modeled by a psychometric function, which characterizes the likelihood of a "yes" response as a function of stimulus strength. These models yield an estimate of the threshold, defined as the stimulus strength at which the observer detects the stimulus with a probability of 50\%.

The most straightforward method for estimating this threshold is the method of constant stimuli, in which stimuli are presented multiple times at several discrete, fixed intensities around the anticipated threshold. This approach yields an accurate estimate of the psychometric function, which is typically modeled using a cumulative distribution function (CDF), such as the Gaussian CDF. However, this method is inefficient and requires a large number of trials, since stimulus levels far from the threshold provide little information. Long test durations negatively affect attention, thereby reducing both the accuracy and precision of the threshold estimates.

To improve efficiency, adaptive algorithms have been developed to reduce the number of stimulus presentations with minimal compromise in accuracy. A commonly used example is the staircase algorithm, in which the stimulus intensity decreases after a "seen" response and increases after a "not seen" response. The step size is halved after each reversal (i.e., a change in response direction). More sophisticated algorithms balance precision and test duration, often incorporating information from adjacent locations in the visual field. A notable example is the Swedish Interactive Threshold Algorithm (SITA) used in the Humphrey Field Analyzer.

At the conclusion of the test, the perimetric sensitivity is recorded in a logarithmic decibel (dB) scale based on the luminance of the stimulus at threshold ($L_{threshold}$). In perimetry, luminance is usually reported in apostilbs rather than $cd/m^2$, where $1 asb = \frac{1}{\pi} cd/m^2$. This scale is defined as the attenuation from the maximum possible stimulus contrast, $L_{max}$:

\begin{equation}
dB = 10\ \rm log \left (\frac{L_{\rm max}}{L_{\rm threshold}} \right)\ .
\end{equation}

$L_{max}$ may differ between parameter types, and the Humphrey Field Analyzer uses 10,000 asb, whereas recent Octopus 900 perimeters commonly use 4000 asb.

The visualFields package was developed to analyze data from static perimetry, where thresholds are measured at fixed, discrete locations in the visual field. Kinetic perimetry, in which a stimulus of constant luminance is moved across the field to identify isosensitivity contours, is outside the scope of this package.

Static perimetry aims to estimate the hill of vision as accurately as possible within the constraints of clinical practice. The most common form is standard automated perimetry (SAP), which is performed inside a dome-shaped device (a Ganzfeld perimeter) to ensure a uniform distance between the eye and all stimulus locations. In contrast, campimetry uses a flat screen and may become more common with the increasing availability of computer screens and virtual reality devices for telemedicine.

\subsection{How is perimetry performed?}

In a typical SAP test, the patient is seated comfortably in front of a perimeter and places their head on a chin rest while looking into a white Ganzfeld dome. The room is dimly lit and quiet, and the patient is briefed by an experienced technician before the examination. During the test, the patient fixates on a central target (e.g., a dot, cross, or diamond), and presses a button whenever a light stimulus is perceived.

Stimuli are presented briefly (typically 100–200 ms), minimizing the opportunity for eye movement. A response is only considered valid if it occurs within a defined time window following the stimulus. A computer presents stimuli at discrete locations in a grid, in randomized order. The stimulus intensity at each location is adjusted adaptively based on the patient' responses. Because of randomization, the patient cannot anticipate where the next stimulus will appear.

The figure below shows the commonly used 24-2 grid of test locations. The gray ellipse represents the anatomical blind spot; the red points fall within this region and are expected to show no sensitivity.

The grid, stimulus size, presentation duration, and intensity algorithm remain constant throughout the test and together define the perimetry program. It is important to note that results from different perimetry programs are not directly comparable.

\subsection{False negatives and false positives}

Deviations from the expected psychometric function may arise due to observer inattention or short-term threshold fluctuations. In diseases like glaucoma, short-term variability is often elevated in damaged regions of the visual field. Response quality is typically assessed by:

\begin{description}
  \item [False negatives (FNR)] Missed responses to stimuli that were previously seen.
  \item [False positives (FPR)] Responses when no stimulus is presented
  \item [Fixation losses] Responses to stimuli presented within the blind spot
\end{description}

These metrics help assess the reliability of a test.

\subsection{Clinical applications of perimetry}

In clinical settings, perimetry is used to detect and monitor visual field defects or scotomas, which can arise from damage anywhere along the visual pathway—from the ocular media (cornea, lens, vitreous), to the retina, optic nerve, or visual cortex.

The spatial pattern of visual field loss often provides diagnostic clues. For instance, a right occipital lobe infarct can result in left homonymous hemianopia—complete loss of the left visual field in both eyes, with a sharp border along the vertical meridian. Conversely, glaucoma typically presents as arcuate defects, often beginning in the upper or lower hemifield, reflecting characteristic damage to the optic nerve head.

Perimetry supports not only diagnosis but also evaluation of functional impact (e.g., fitness to drive) and disease progression monitoring.

%% -- Methods ------------------------------------------------------------------

\section{Methods} \label{sec:methods}

\subsection{Installation}

The \pkg{visualFields} package is available from CRAN under the Apache 2.0 license and can be installed with:

<<eval=FALSE>>=
install.packages("visualFields")
@

The latest development version can be installed from GitHub using:

<<eval=FALSE>>=
library(devtools)
install_github("imarinfr/vf1/source")
@

\subsection{Grid patterns}

Test locations are defined by location maps (\code{locmaps}), which specify the spatial coordinates of stimulus presentation points in degrees of visual angle. These maps reflect the layout used by visual field testing devices. When analyzing data, the correct \code{locmap} must be specified.

<<>>=
names(locmaps)
@

By default, the \code{24-2} pattern is used, consisting of 54 regularly spaced locations across the central visual field.

<<>>=
default_locmap <- getlocmap()
str(default_locmap, strict.width = "wrap")
@

Each \code{locmap} contains:
\begin{itemize}
  \item A coordinate table with $x$- and $y$-positions (in degrees),
  \item A vector \code{bs} indicating locations corresponding to the physiological blind spot.
\end{itemize}

Other patterns are available in the package, including the \code{10-2} grid.

<<>>=
ten_two_locmap <- locmaps$p10d2
str(ten_two_locmap, strict.width = "wrap")
@

If the blind spot is outside the tested region (e.g., in the 10-2 pattern) or is skipped (e.g., in the Octopus G1), \code{bs} is set to \code{numeric(0)}.

\subsection{Working with visual field data}

The package includes example data from a glaucoma patient (\code{vfpwgSunyiu24d2}), consisting of 42 visual field tests.

<<>>=
str(vfpwgSunyiu24d2[, 1:10], strict.width = "wrap")
@

Each row represents a monocular test. The eye is labeled using Latin abbreviations: 

\begin{description}
\item [OD] oculus dexter -> right eye
\item [OS] oculus sinister -> left eye
\item [OU] oculus uterque -> both eyes
\end{description}

Initial columns store patient information and reliability metrics. Actual test values (sensitivities) begin at the column number stored in the \code{locini} variable. The function \code{getvfcols()} returns the numbers of the columns that contain the actual test values at the locations \code{l1} to \code{ln}, where 1..n corresponds to the rows of \code{coords} in the \code{locmaps}.

<<>>=
getlocini()
vfpwgSunyiu24d2[1, getvfcols()]
@

No columns should appear after these test values. Use \code{vffilter()} to select specific tests and \code{vfisvalid()} to verify data structure:

<<>>=
example1 <- vffilter(vfpwgSunyiu24d2, id == "sample1" & eye == "OD")[1, ]
example1$newVariable <- "invalid"
vfisvalid(example1)
@

The \code{vfread()} and \code{vfread()} functions can be used to read and write data in this format from a CSV file. However, these functions cannot be used to import native data from the perimeters. For this, the package includes functions to import data from the Octopus 900 and the Humphrey Field Analyzer.

\begin{itemize}
  \item \code{loadoctopus()} for Octopus 900 (Haag-Streit, Switzerland)
  \item \code{loadhfaxml()}, \code{loadhfadicom()}, \code{loadhfaxmlbatch()}, \code{loadhfadicombatch()} for the Humphrey Field Analyzer (Zeiss)
\end{itemize}

Detailed examples for exporting data from the Octopus 900 and using \code{loadoctopus()} can be found here: \url{https://huchzi.github.io/vf1/loadOctopus.html}.

\subsection{Normative data}

Sensitivity is measured in decibels (dB), derived from contrast thresholds. Sensitivity varies:
\begin{itemize}
  \item Intra-individually (test-retest variability),
  \item Inter-individually (e.g., with age),
  \item By eccentricity.
\end{itemize}

To detect abnormal points, we compare observed sensitivities with an age-corrected mean normal field. These comparisons require a \emph{normative dataset} and location-wise \emph{age-linear models}.

The difference between observed and normal values is the \emph{total deviation (TD)}. In this package, negative values indicate lower-than-normal sensitivity, following the Humphrey Field Analyzer convention. For perimeters that reverse this definition (e.g., Octopus), users must invert the sign.

To reveal focal defects masked by generalized depression (e.g., due to cataract), a \emph{pattern deviation (PD)} is computed by subtracting a general height (GH) estimate from the TD. For 24-2, GH is defined as the 7th highest TD value.

The package includes three datasets with visual fields from normal observers:

\begin{itemize}
  \item SUNY-IU dataset (\code{vfctrsunyiu_24d2}) for 24-2 with Goldmann size III \citep{IMF:Mar2013b}
  \item Iowa PC26 (\code{vfctriowa_PC26}) and Peri (\code{vfctrsunyiu_10d2}) datasets for Goldmann size V \citep{IMF:Marin2018c}
\end{itemize}

<<>>=
str(
  vfctrSunyiu24d2, 
  strict.width = "wrap")
@

The normal values derived from these data can be found in \code{normvals}.

<<>>=
str(
  normvals$sunyiu_24d2, 
  max.level = 1, strict.width = "wrap")
@

To build the normative model for a custom dataset, use \code{nvgenerate()}.

<<>>=
nv <- nvgenerate(vfctrSunyiu24d2)
setnv(nv)
@

The normative object \code{nv} includes coefficients of age-linear models (\code{nv$agem$coeff}).:

<<>>=
head(nv$agem$coeff, 5)
@

It also contains a function that calculates normal sensitivity for a given age (\code{nv$agem$model()}), Thus, the expected field at age 50 is:

<<>>=
age <- 50
nv$agem$model(age)
@

Blind spot locations are returned as \code{NA}.

%% -- Results ------------------------------------------------------------------

\section{Results} \label{sec:results}

\subsection{Metric for individual locations}

\subsubsection{Total deviation}

Total deviation is the difference between observed and expected sensitivity. Here, it is shown how it is calculated.

<<>>=
example2 <- vffilter(vfpwgRetest24d2, id == "1", eye == "OD")[1, ]
age <- example2$age[1]
normal_values <- getnv()$agem$model(age)[1:10]
example2[, getvfcols()[1:10]] - normal_values
@

To calculate the total deviation, use the function from the \code{nv} object:

<<>>=
td <- gettd(example2)
print(td[, getvfcols()[1:10]])
@

\subsubsection{Pattern deviation}

Pattern deviation is obtained by subtracting the general height:

<<>>=
gh <- getgh(td)
pattern_defects <- td[, getvfcols()[1:10]] - gh
@

Or automatically:

<<>>=
pd <- getpd(td)
print(pd[, getvfcols()[1:10]])
@

\subsubsection{Probability values}

Even healthy eyes may show small deviations. To interpret these values, we calculate the probability that a deviation reflects pathology using empirical distributions from the normative data \citep{IMF:Hei1987a, IMF:Heijl1991}.

<<>>=
print(getnv()$lut$probs)
print(gettdp(td)[, getvfcols()[1:10]])
print(getpdp(pd)[, getvfcols()[1:10]])
@

Probabilities less than 0.05 suggest the location is likely abnormal.

\subsection{Global indices}

Global indices summarize the visual field by applying summary statistics on sensitivity, total deviation or pattern deviation. These indices are calculated using \code{getgl()}. The resulting \code{data.frame} also contains the patient's age, eye, and date of the test.

<<>>=
gl <- getgl(example2)
str(gl, max.level = 1, strict.width = "wrap")
gl[, -(1:getlocini() - 1)]
@

\begin{description}
  \item [msens]: Mean sensitivity (MS)
  \item [ssens]: SD of sensitivity
  \item [tmd]: Mean total deviation (MD)
  \item [tsd]: SD of total deviation
  \item [pmd]: Mean pattern deviation
  \item [psd]: SD of pattern deviation
  \item [gh]: General height
  \item [vfi]: Visual field index (VFI) \citep{IMF:Ben2008}
\end{description}

Associated probabilities are calculated with \code{getglp()}:

<<>>=
getglp(gl)[, -(1:getlocini() - 1)]
@

\subsection{Plotting the visual field}

There are various ways to visualize perimetric data. These visualizations should be interpreted as simplified representations of the hill of vision.

Standard displays include color-coded probability maps, graphical representations of the blind spot, and border enhancements that encode diagnostic information. For example, \code{vfplot} with the argument 	\code{"td"} generates plots that require normative data on differential light sensitivity and the probabilities of deviations from these norms. This information is included for the Humphrey Field Analyzer patterns but must be provided separately for other perimeters.

By convention, when displaying both eyes side-by-side, the left eye appears on the left and the right eye on the right. This reflects the patient's visual fields, with blind spots positioned temporally. This is in contrast to structural imaging, which is typically displayed as viewed by the examiner.

It is important to specify the type of data represented in the visual field tables. The default is set to \emph{total deviation} (	exttt{td}).

<<fig=TRUE>>=

vfplot(example2)

@

In this plot, yellow indicates a probability $<$ 0.05, orange $<$ 0.02, red $<$ 0.01, and dark red $<$ 0.005.

A gallery of all available plot types is available at \url{https://rpubs.com/huchzi/1307927}.

\subsection{Longitudinal analyses}

Monitoring visual function over time is a key application of perimetry in managing glaucoma.

Sensitivity measurements naturally fluctuate between tests. This variability increases in eyes with glaucoma, especially at affected locations. Hence, two tests showing differences cannot alone indicate progression. Instead, statistical analysis across a series of tests is required to detect significant change.

Progression analysis methods fall into two broad categories: \emph{event-based} and \emph{trend-based} approaches. These may further be classified by their reliance on population-based normative data, mathematical models of progression, or model-free methods.

Event-based methods (not implemented in \pkg{visualFields}) compare a test to a baseline and flag change if the sensitivity reduction is unlikely ($p < 0.05$) based on normative variability, and if confirmed in subsequent tests.

Trend-based methods assume slower change and analyze all tests in a series. Though less intuitive for clinicians, these methods are statistically robust and reflect real-world progression patterns.

To illustrate, we select a time series of 27 visual fields from one eye:

<<>>=
example3 <- vffilter(vfpwgSunyiu24d2, id == "sample1" & eye == "OS")
@

\subsubsection{Plotting progression}

Example plots used for monitoring progression are available in the gallery: \url{https://rpubs.com/huchzi/1307927}.

\subsubsection{Statistical analysis}

Statistical progression analysis may be performed via:
\begin{itemize}
\item \emph{Global linear regression} (e.g., on MD)
\item \emph{Pointwise linear regression} (PLR)
\end{itemize}

The \code{glr()} function estimates a regression model for global indices. For the example series, the patient loses \code{getgl(example3)} dB/year, with $p < getgl(example3)$.

%\code{r round(abs(glr(getgl(example3))$sl), 2)}
%\code{r format(ceiling(1000 * glr(getgl(example3))$pval) / 1000, nsmall = 3)}

Pointwise linear regression is performed using \code{plr()}, resulting in one slope and $p$-value per location:

<<>>=
prog1 <- plr(example3)
str(prog1, max.level = 1, strict.width = "wrap")
print(sum(prog1$pval[-getlocmap()$bs] < 0.05))
@

To visualize these results:

<<fig=TRUE>>=
vfplotplr(example3)
@

In this display, yellow indicates $p < 0.05$, orange $< 0.02$, red $< 0.01$, and dark red $< 0.005$.

PLR-based criteria (e.g., 3 or more progressing points) are clinically useful but lack explicit control for specificity. \emph{Permutation of Pointwise Linear Regression (PoPLR)} addresses this by combining the 52 $p$-values into a single $S$-statistic (per Fisher's method) and comparing it to a null distribution from permuted series \citep{IMF:Ole2012,IMF:Marin2021}.

<<>>=
prog2 <- poplr(example3)
str(prog2, max.level = 1, strict.width = "wrap")
@

The null distribution and observed statistic:

<<>>=
print(prog2$csr)
print(prog2$csrp)
print(prog2$csl)
print(prog2$cslp)
@

Visualization:

<<>>=
hist(prog2$cstats$cslall, xlab = "S-values", xlim = c(0, 200))
abline(v = prog2$csl, col = "red", lwd = 2)
@

\subsection{Automated reports}

Static reports and Shiny applications can be generated:

<<eval=FALSE>>=
vfsfa(example2, file = "test.pdf")
vfsfashiny(example2)
@

Automated reports can also be generated for progression series:

<<eval = FALSE>>=
vfspa(example3, file = "test.pdf")
vfspashiny(example3)
@

\subsection{Structure-function correlation}

Anatomical modeling aids comparison between structural imaging and functional testing in glaucoma.

The ganglion cell bodies stimulated by a visual field location are displaced from the photoreceptor input region, especially in the central field. This displacement is modeled in \pkg{visualFields} based on Montesano et al. \citep{IMF:Montesano2020}.

<<>>=
locmap <- locmaps$p10d2
gcmap <- vf2gc(locmap$coord)
@

A plot of the 10-2 grid and corresponding ganglion cell bodies:

<<echo=TRUE, fig=TRUE>>=

plot_grid <- function() {
  lines(-c(10, 0.5), c(0, 0), lwd = 2, col = "lightgray")
  lines(c(0.5, 10), c(0, 0), lwd = 2, col = "lightgray")
  lines(c(0, 0), -c(10, 0.5), lwd = 2, col = "lightgray")
  lines(c(0, 0), c(0.5, 10), lwd = 2, col = "lightgray")
  lines(x = c(-0.5, 0.5), y = c(0, 0), lwd = 1.5)
  lines(x = c(0, 0), y = c(-0.5, 0.5), lwd = 1.5)
  for(r in c(2.5, 5, 7.5, 10))
    draw.circle(0, 0, r, lwd = 2, col = NA, border = "lightgray")
  text(x = 11, y = 0, "10\u00b0", adj = c(0, 0.5))
  text(x = 0, y = 11, "10\u00b0", adj = c(0.5, 0))
}

par(mfrow = c(1, 2))
plot(x = locmap$coord$x, 
     y = locmap$coord$y, 
     type = "n", 
     asp = 1,
     xlim = c(-10, 12),
     ylim = c(-10, 10),
     yaxt = 'n',
     xaxt = 'n',
     bty = 'n',
     xlab = "",
     ylab = "",
     main = "10-2 grid of test locations")
plot_grid()
points(x = locmap$coord$x, y = locmap$coord$y, bg = "white")

plot(x = locmap$coord$x, 
     y = locmap$coord$y, 
     type = "n", 
     asp = 1,
     xlim = c(-10, 12),
     ylim = c(-10, 10),
     yaxt = 'n',
     xaxt = 'n',
     bty = 'n',
     xlab = "",
     ylab = "",
     main = "Corresponding ganglion cells")
plot_grid()
points(x = gcmap$x, y = gcmap$y, pch = 21, bg = "white")
@

Nerve fiber paths from each location to the optic disc can also be modeled, using Jansonius' framework \citep{IMF:Jan2009, IMF:Jan2012}.

<<>>=
psi23 <- loc2psi(vf2gc(locmaps$p24d2$coord[23,]))
ang23 <- psi2oct(psi23)

ang23 - 360
@

\subsection{DICOM support}

DICOM standards for static perimetry are defined in Supplement 146. The package includes preliminary functions for reading HFA-generated DICOM files, but exporting to DICOM is not yet supported.

\subsection{Support for other perimeters}

Perimetry conventions vary across manufacturers. The package includes normative data for Humphrey 24-2 (Goldmann III) and Iowa full-field perimetry with Goldmann V \citep{IMF:Marin2018c}. The \code{loadoctopus} function provides partial support for Octopus G1 fields, but normative support is incomplete.

Users may provide custom normative datasets using \code{nvgenerate()} and \code{setnv()}.

\subsection{Setting the environment}

Each analysis requires environment variables:
\begin{itemize}
\item \code{nv}: normative values
\item \code{locmap}: test grid
\item \code{gpar}: graphical parameters
\item \code{locini}: first sensitivity column
\end{itemize}

<<>>=
names(locmaps)
names(normvals)
names(gpars)
@

These must be consistent. For example:

<<>>=
setlocmap(locmaps$pPC26)
setnv(normvals$iowa_PC26_pw)
setgpar(gpars$pPC26)
setlocini(24)
@

To reset to default (Humphrey 24-2):

<<>>=
setdefaults()
@

\subsection{Custom configurations}

Custom test structures must:
\begin{itemize}
\item Start with columns: \code{id}, \code{eye}, \code{date}, \code{time}
\item Include \code{age}, optionally \code{fpr}, \code{fnr}, \code{fl}
\item Have sensitivity columns named \code{l1}--\code{lk}
\end{itemize}

To import custom locations:

<<eval=FALSE>>=
g1 <- list(coord = read.csv("./g1.csv"), 
           name = "G1", 
           desc = "G1 map", 
           bs = numeric())
setlocmap(g1)
setgpar(vfgpar(g1$coord))
vfplot(vfread("./vfg1.csv"), type = "s")
@

If normative data exist, use \code{vfread()}, \code{nvgenerate()}, and \code{setnv()} to enable deviation computations.

%% -- Summary and discussion ---------------------------------------------------

\section{Summary and discussion} \label{sec:summary}

The \pkg{visualFields} package provides a robust suite of tools for statistical analysis and visualization of visual field data. It supports single-field reports, progression analysis, anatomical modeling, and customization for non-standard perimeters. As part of the Open Perimetry Initiative, it aims to foster reproducibility and transparency in perimetry research. Future development could focus on expanding normative datasets, full DICOM integration, and extended support for additional testing strategies and devices.

%% -- Acknowledgements ---------------------------------------------------

\section{Acknowledgments}
The visualField's package would not have existed if it wasn't for the important contribution of many contributors who partially funded the development of the package, and/or contributed datasets, code, and above all insight on visual fields analysis and about perimetry and visual sciences at large. Huge thanks to:
  - William H Swanson
  - Michael Wall
  - Andrew Turpin
  - Paul H Artes
  - Giovanni Montesano
  - Mitchell W Dul

%% -- Bibliography -------------------------------------------------------------

\bibliography{references}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

\begin{leftbar}
Appendices can be included after the bibliography (with a page break). Each
section within the appendix should have a proper section title (rather than
just \emph{Appendix}).

For more technical style details, please check out JSS's style FAQ at
\url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
which includes the following topics:
\begin{itemize}
  \item Title vs.\ sentence case.
  \item Graphics formatting.
  \item Naming conventions.
  \item Turning JSS manuscripts into \proglang{R} package vignettes.
  \item Trouble shooting.
  \item Many other potentially helpful details\dots
\end{itemize}
\end{leftbar}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

\begin{leftbar}
References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
\verb|\citealp| etc.\ (and never hard-coded). This commands yield different
formats of author-year citations and allow to include additional details (e.g.,
pages, chapters, \dots) in brackets. In case you are not familiar with these
commands see the JSS style FAQ for details.

Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
when acquiring the entries automatically from mixed online sources. However,
it is important that informations are complete and presented in a consistent
style to avoid confusions. JSS requires the following format.
\begin{itemize}
  \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
    be used in the references.
  \item Titles should be in title case.
  \item Journal titles should not be abbreviated and in title case.
  \item DOIs should be included where available.
  \item Software should be properly cited as well. For \proglang{R} packages
    \code{citation("pkgname")} typically provides a good starting point.
\end{itemize}
\end{leftbar}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
